"""\nComando Django para treinar modelo LTR com XGBoost.\n\nUso:\n    python manage.py train_ltr --days=30 --save-model\n"""\nimport os\nimport pickle\nfrom datetime import datetime, timedelta\nfrom typing import List, Tuple, Any\n\nimport xgboost as xgb\nimport pandas as pd\nfrom django.core.management.base import BaseCommand\nfrom django.utils import timezone\n\nfrom apps.logs.models import SearchLog, ImpressionLog\nfrom apps.ltr.utils import extract_query_features, calculate_ndcg\n\n\nclass Command(BaseCommand):\n    help = 'Treina modelo Learning-to-Rank usando dados de cliques'\n    \n    def add_arguments(self, parser):\n        parser.add_argument(\n            '--days', \n            type=int, \n            default=30,\n            help='Número de dias de dados para treinamento'\n        )\n        parser.add_argument(\n            '--save-model',\n            action='store_true',\n            help='Salvar modelo treinado em arquivo'\n        )\n        parser.add_argument(\n            '--test-split',\n            type=float,\n            default=0.2,\n            help='Proporção de dados para teste (0.0-1.0)'\n        )\n        parser.add_argument(\n            '--min-impressions',\n            type=int,\n            default=5,\n            help='Mínimo de impressões por query para incluir no treino'\n        )\n    \n    def handle(self, *args, **options):\n        days = options['days']\n        save_model = options['save_model']\n        test_split = options['test_split']\n        min_impressions = options['min_impressions']\n        \n        self.stdout.write(f'Coletando dados dos últimos {days} dias...')\n        \n        # 1. Coleta dados de treinamento\n        features, labels, groups = self._collect_training_data(\n            days, min_impressions\n        )\n        \n        if not features:\n            self.stdout.write(\n                self.style.ERROR('Nenhum dado de treinamento encontrado!')\n            )\n            return\n            \n        self.stdout.write(\n            f'Coletados {len(features)} exemplos de {len(groups)} queries'\n        )\n        \n        # 2. Divide dados em treino/teste\n        train_data, test_data = self._split_data(\n            features, labels, groups, test_split\n        )\n        \n        # 3. Treina modelo\n        model = self._train_model(train_data)\n        \n        # 4. Avalia modelo\n        if test_data:\n            ndcg = self._evaluate_model(model, test_data)\n            self.stdout.write(\n                self.style.SUCCESS(f'nDCG@10 no teste: {ndcg:.4f}')\n            )\n        \n        # 5. Salva modelo se solicitado\n        if save_model:\n            model_path = self._save_model(model)\n            self.stdout.write(\n                self.style.SUCCESS(f'Modelo salvo em: {model_path}')\n            )\n    \n    def _collect_training_data(\n        self, days: int, min_impressions: int\n    ) -> Tuple[List[List[float]], List[float], List[int]]:\n        \"\"\"Coleta features, labels e groups dos logs.\"\"\"\n        cutoff_date = timezone.now() - timedelta(days=days)\n        \n        # Busca queries com impressões suficientes\n        search_logs = SearchLog.objects.filter(\n            created_at__gte=cutoff_date\n        ).prefetch_related('impressions__click')\n        \n        all_features = []\n        all_labels = []\n        all_groups = []\n        \n        for search_log in search_logs:\n            impressions = list(search_log.impressions.all())\n            \n            if len(impressions) < min_impressions:\n                continue\n                \n            # Extrai features e labels desta query\n            query_features, query_labels = extract_query_features(\n                search_log, impressions\n            )\n            \n            if not query_features:\n                continue\n                \n            all_features.extend(query_features)\n            all_labels.extend(query_labels)\n            all_groups.append(len(query_features))  # tamanho do grupo\n        \n        return all_features, all_labels, all_groups\n    \n    def _split_data(\n        self, \n        features: List[List[float]], \n        labels: List[float], \n        groups: List[int],\n        test_split: float\n    ) -> Tuple[dict, dict]:\n        \"\"\"Divide dados em treino/teste respeitando grupos.\"\"\"\n        if test_split <= 0:\n            return {\n                'features': features,\n                'labels': labels, \n                'groups': groups\n            }, None\n            \n        # Calcula split por número de grupos\n        num_test_groups = max(1, int(len(groups) * test_split))\n        \n        # Separa últimos grupos para teste\n        test_groups = groups[-num_test_groups:]\n        train_groups = groups[:-num_test_groups]\n        \n        # Calcula índices\n        train_size = sum(train_groups)\n        \n        train_data = {\n            'features': features[:train_size],\n            'labels': labels[:train_size],\n            'groups': train_groups\n        }\n        \n        test_data = {\n            'features': features[train_size:],\n            'labels': labels[train_size:],\n            'groups': test_groups\n        }\n        \n        return train_data, test_data\n    \n    def _train_model(self, train_data: dict) -> xgb.XGBRanker:\n        \"\"\"Treina modelo XGBoost ranking.\"\"\"\n        self.stdout.write('Treinando modelo XGBoost...')\n        \n        model = xgb.XGBRanker(\n            objective='rank:pairwise',\n            learning_rate=0.1,\n            n_estimators=100,\n            max_depth=6,\n            subsample=0.8,\n            colsample_bytree=0.8,\n            random_state=42\n        )\n        \n        # Converte para DataFrame\n        df = pd.DataFrame(train_data['features'], columns=[\n            'sim_semantico',\n            'score_confianca',\n            'score_avaliacao', \n            'score_engajamento',\n            'score_proximidade',\n            'score_qualificacao'  # Nova feature acadêmica\n        ])\n        \n        model.fit(\n            df,\n            train_data['labels'],\n            group=train_data['groups'],\n            verbose=False\n        )\n        \n        return model\n    \n    def _evaluate_model(self, model: xgb.XGBRanker, test_data: dict) -> float:\n        \"\"\"Avalia modelo usando nDCG@10.\"\"\"\n        if not test_data:\n            return 0.0\n            \n        df = pd.DataFrame(test_data['features'], columns=[\n            'sim_semantico',\n            'score_confianca', \n            'score_avaliacao',\n            'score_engajamento',\n            'score_proximidade',\n            'score_qualificacao'\n        ])\n        \n        predictions = model.predict(df)\n        \n        # Calcula nDCG por grupo\n        ndcg_scores = []\n        start_idx = 0\n        \n        for group_size in test_data['groups']:\n            end_idx = start_idx + group_size\n            \n            group_labels = test_data['labels'][start_idx:end_idx]\n            group_preds = predictions[start_idx:end_idx]\n            \n            ndcg = calculate_ndcg(group_labels, group_preds, k=10)\n            ndcg_scores.append(ndcg)\n            \n            start_idx = end_idx\n        \n        return sum(ndcg_scores) / len(ndcg_scores) if ndcg_scores else 0.0\n    \n    def _save_model(self, model: xgb.XGBRanker) -> str:\n        \"\"\"Salva modelo treinado.\"\"\"\n        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n        model_dir = 'models/ltr/'\n        os.makedirs(model_dir, exist_ok=True)\n        \n        model_path = f'{model_dir}/xgb_ranker_{timestamp}.pkl'\n        \n        with open(model_path, 'wb') as f:\n            pickle.dump(model, f)\n            \n        # Salva também versão \"atual\" \n        current_path = f'{model_dir}/current_model.pkl'\n        with open(current_path, 'wb') as f:\n            pickle.dump(model, f)\n            \n        return model_path